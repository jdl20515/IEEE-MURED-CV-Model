{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdl20515/IEEE-MURED-CV-Model/blob/main/Investigating_the_Effectiveness_of_Convolutional_Neural_Networks_on_Retinal_Disease_Diagnosis_Using_the_MURED_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the model that was used in the paper *Investigating the Effectiveness of Convolutional Neural Networks on Retinal Disease Diagnosis Using the MURED Dataset*, published in the 2024 IEEE IATMSI conference.\n",
        "\n",
        "The full paper can be found at IEEE Explore: https://ieeexplore.ieee.org/document/10502464.\n",
        "\n",
        "Jose David Lomelin; An Tran; Saumik Das; Lakshmisaketh Alluri; Rushil Challa; Sahil Sanjeev Narula; Aaditiya Jaganathan"
      ],
      "metadata": {
        "id": "EDEFl1T1zaCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFS-nYrQOedo"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow_addons\n",
        "!pip install tensorflow-ranking\n",
        "#pip install pyyaml h5py\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_ranking as tfr\n",
        "import tensorflow_addons as tfa\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import keras.api._v2.keras as keras\n",
        "import random\n",
        "\n",
        "from tensorflow.keras import Model, Input, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, Flatten, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2M, preprocess_input\n",
        "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.applications.resnet_v2  import ResNet50V2, preprocess_input\n",
        "from keras.layers import Dense, Flatten, Concatenate\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "\n",
        "\n",
        "# Run If Vgg16 does not work\n",
        "# !pip uninstall tensorflow\n",
        "# !pip install tensorflow==2.9.1\n",
        "\n",
        "# Run if tfr/tfad does not work\n",
        "#!pip install tensorflow-ranking\n",
        "#!pip install tensorflow_addons\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKWycWElOrc3"
      },
      "outputs": [],
      "source": [
        "# Loads image in from the set image path\n",
        "#from PIL import\n",
        "# Mount and Set Dataset Path (MURED)\n",
        "# Can vary from 520x520 to 3400x2800\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "INPUT = (384, 384, )\n",
        "\n",
        "#Define Noise Function\n",
        "def add_noise(img):\n",
        "    '''Add random noise to an image'''\n",
        "    VARIABILITY = 6\n",
        "\n",
        "    #Add noise only 20% of the time\n",
        "    rand = random.random()\n",
        "    if rand > .8:\n",
        "      deviation = VARIABILITY*random.random()\n",
        "      noise = np.random.normal(0, deviation, img.shape)\n",
        "      img += noise\n",
        "      np.clip(img, 0., 255.)\n",
        "\n",
        "    return img\n",
        "\n",
        "# Add Data Augmentations Dataset\n",
        "train = ImageDataGenerator(rescale = 1/255, width_shift_range=[-10, 10], height_shift_range=[-10, 10], brightness_range=[0.8, 1.2], horizontal_flip = True, zoom_range=.1, samplewise_center=True, samplewise_std_normalization=True, zca_whitening=True, preprocessing_function=add_noise,)\n",
        "val = ImageDataGenerator(rescale = 1/255, width_shift_range=[-10, 10], height_shift_range=[-10, 10], brightness_range=[0.8, 1.2], horizontal_flip = True, zoom_range=.1, samplewise_center=True, samplewise_std_normalization=True, zca_whitening=True, preprocessing_function=add_noise,)\n",
        "sample = ImageDataGenerator(rescale = 1/255)\n",
        "\n",
        "\n",
        "#Define Training and Validation Set\n",
        "training_set = train.flow_from_directory(\"/content/drive/MyDrive/MURED/train\",\n",
        "                                         target_size = INPUT,\n",
        "                                         batch_size = 32,\n",
        "                                         class_mode = \"categorical\",\n",
        "                                         )\n",
        "\n",
        "validation_set = val.flow_from_directory(\"/content/drive/MyDrive/MURED/val\",\n",
        "                                         target_size = INPUT,\n",
        "                                         batch_size = 32,\n",
        "                                         class_mode = \"categorical\",\n",
        "\n",
        "                                         )\n",
        "\n",
        "#Fit Generators\n",
        "from numpy import asarray\n",
        "from PIL import Image\n",
        "spath = Image.open(\"/content/drive/MyDrive/MURED/small_sample/sample/23.png\")\n",
        "spath = spath.resize((384, 384))\n",
        "simage = asarray(spath)\n",
        "simage = simage.astype('float32')\n",
        "\n",
        "\n",
        "simage = np.expand_dims(simage, axis=0)\n",
        "print(simage.shape)\n",
        "\n",
        "#train.fit(simage)\n",
        "#val.fit(simage)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVB8-tAvOu57"
      },
      "outputs": [],
      "source": [
        "# Make Model\n",
        "\n",
        "# Make a copy of this model, with original idea (run xception, then resize with conv layers to fit the original image size, then pass into rest of models. Compare to this model's efficiency/time)\n",
        "# DONT FORGET TO FREEZE NON FC LAYERS\n",
        "# DONT FORGET TO DO CHECKPOINTS\n",
        "\n",
        "INPUT = (384, 384, 3)\n",
        "inputlayer = Input(shape=INPUT)\n",
        "\n",
        "# Initializing Pretrained Models and Setting Convolutions to the Same Size\n",
        "xception_model = Xception(weights = \"imagenet\", include_top=False, input_tensor=inputlayer)\n",
        "\n",
        "conv1 = xception_model.output\n",
        "conv1 = Conv2D(2048, (1, 1), activation=\"relu\", padding=\"valid\", strides=1)(conv1)\n",
        "#conv1 = MaxPooling2D(pool_size=(2,2), strides=(1,1), padding=\"valid\")(conv1)\n",
        "\n",
        "\n",
        "resNet_model = ResNet50V2(weights = \"imagenet\", include_top=False, input_tensor=inputlayer)\n",
        "# for layers in resNet_model.layers[:-5]:\n",
        "#   layers.trainable = False\n",
        "conv2 = resNet_model.output\n",
        "conv2 = Conv2D(2048, (1, 1), activation=\"relu\", padding=\"valid\", strides=1)(conv2)\n",
        "#conv2 = MaxPooling2D(pool_size=(2,2), strides=(1,1), padding=\"valid\")(conv2)\n",
        "\n",
        "\n",
        "\n",
        "# Concatenating the Base Models' Convolutions\n",
        "concat1 = tf.concat([conv1, conv2], 3)\n",
        "#concat1 = Flatten()(concat1)\n",
        "\n",
        "\n",
        "# Convolutional and Pooling Layers to Base Models' Convolutions\n",
        "\n",
        "# Xception Model\n",
        "conv1 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", strides=1)(conv1)\n",
        "conv1 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')(conv1)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "\n",
        "conv1 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", strides=1)(conv1)\n",
        "conv1 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')(conv1)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "\n",
        "conv1 = Conv2D(2048, (1, 1), activation=\"relu\", padding=\"same\", strides=1)(conv1)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "\n",
        "# ResNet Model\n",
        "conv2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", strides=1)(conv2)\n",
        "conv2 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')(conv2)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "\n",
        "conv2 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", strides=1)(conv2)\n",
        "conv2 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')(conv2)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "\n",
        "conv2 = Conv2D(2048, (1, 1), activation=\"relu\", padding=\"same\", strides=1)(conv2)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "\n",
        "# Concatenating the new Convolutions\n",
        "concat2 = tf.concat([conv1, conv2], 3)\n",
        "#concat2 = Flatten()(concat2)\n",
        "\n",
        "\n",
        "concat3 = tf.concat([concat1, concat2], 3)\n",
        "\n",
        "\n",
        "#concat3 = tf.reshape(xceptionConv, shape=[15, 15, 2048])\n",
        "concat3 = MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid')(concat3)\n",
        "print(concat3)\n",
        "concat3 = Conv2D(4096, (1, 1), activation=\"relu\", padding=\"valid\", strides=1)(concat3)\n",
        "\n",
        "print(concat3)\n",
        "concat3 = Flatten()(concat3)\n",
        "print(concat3)\n",
        "\n",
        "\n",
        "\n",
        "fc1 = Dense(2056, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.05, l2=0.05))(concat3)\n",
        "fc1 = Dropout(0.05)(fc1)\n",
        "fc1 = BatchNormalization()(fc1)\n",
        "\n",
        "fc2 = Dense(2056, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.05, l2=0.05))(fc1)\n",
        "fc2 = Dropout(0.05)(fc2)\n",
        "fc2 = BatchNormalization()(fc2)\n",
        "\n",
        "fc3 = Dense(1024, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.05, l2=0.05))(fc2)\n",
        "fc3 = Dropout(0.05)(fc3)\n",
        "fc3 = BatchNormalization()(fc3)\n",
        "\n",
        "fc4 = Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.05, l2=0.05))(fc3)\n",
        "fc4 = Dropout(0.05)(fc4)\n",
        "fc4 = BatchNormalization()(fc4)\n",
        "\n",
        "\n",
        "\n",
        "preds = Dense(19, activation=\"softmax\")(fc4)\n",
        "\n",
        "model = Model(inputs = inputlayer, outputs = preds)\n",
        "\n",
        "\n",
        "# Freeze Pretrained Models\n",
        "for layers in model.layers[:-42]:\n",
        "   layers.trainable = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWHGjF1UOxQL"
      },
      "outputs": [],
      "source": [
        "#@title Default title text\n",
        "# config = tf.compat.v1.ConfigProto()\n",
        "# config.gpu_options.allow_growth=True\n",
        "# sess = tf.compat.v1.Session(config=config)\n",
        "# Train and run model\n",
        "\n",
        "'''\n",
        "Importance Level    Hyperparameters\n",
        "\n",
        "First               Learning Rate Alpha\n",
        "\n",
        "Second              preprocessing\n",
        "                    mini-batch size\n",
        "                    number of hidden units/layers\n",
        "\n",
        "Third               Adam beta1, beta2 (Between 1-0, higher means faster learning)\n",
        "                    Dropout and L1/L2 regulrization\n",
        "'''\n",
        "epochs = 10\n",
        "optimizer = Adam(learning_rate=0.000000003,\n",
        "                 beta_1=0.9,\n",
        "                 beta_2=0.999,)\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics = [\n",
        "                                                                                   tf.keras.metrics.CategoricalAccuracy(),\n",
        "                                                                                   tf.keras.metrics.AUC(),\n",
        "                                                                                   tfr.keras.metrics.MeanAveragePrecisionMetric(),\n",
        "                                                                                   tfa.metrics.F1Score(num_classes=19)])\n",
        "\n",
        "\n",
        "# Last train and load weights\n",
        "# checkpoint_path = \"/content/drive/MyDrive/training_1/cp-0007.ckpt\"\n",
        "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "\n",
        "# !ls /content/drive/MyDrive/training_1\n",
        "# latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "# print(latest)\n",
        "model.load_weights(\"/content/drive/MyDrive/training_6/cp.hdf5\")\n",
        "\n",
        "# New train/weights\n",
        "filepath = \"/content/drive/MyDrive/training_6/cp.hdf5\"\n",
        "#new_dir = os.path.dirname(new_path)\n",
        "\n",
        "# Create a callback that saves the model's weights every epoch\n",
        "#epoch_size = 128\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath,\n",
        "    verbose=1,\n",
        "    monitor='val_Categorical_Accuracy',\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    )\n",
        "\n",
        "# Fit model\n",
        "# model.fit(training_set,\n",
        "#           validation_data = validation_set,\n",
        "#           epochs = 1,\n",
        "#           #epochs = epochs,\n",
        "#           callbacks=[cp_callback]\n",
        "#           )\n",
        "\n",
        "#Evaluate model\n",
        "model.evaluate(validation_set,\n",
        "\n",
        "          #epochs = 1,\n",
        "          #epochs = epochs,\n",
        "          #callbacks=[cp_callback]\n",
        "          )\n",
        "print(model.metrics_names)\n",
        "print(model.metrics)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}